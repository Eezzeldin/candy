{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3Mv6OmDu7SKDR/df169Kh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eezzeldin/candy/blob/main/candy_interdependence_cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aelgRw8vibQU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yi9NOU0KihL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict, StratifiedKFold\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/candy-data.csv')  # Replace with your dataset path\n",
        "\n",
        "# Define binary and continuous feature columns\n",
        "binary_columns = df.select_dtypes(include='int64').columns.drop('bar')\n",
        "continuous_columns = ['sugarpercent', 'pricepercent', 'winpercent']\n",
        "binary_and_continuous_columns = binary_columns.append(pd.Index(continuous_columns))\n",
        "\n",
        "# Hyperparameter grid for RandomForest\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 500],\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Function to compute tuned feature importances and balanced accuracy\n",
        "def compute_tuned_accuracy_with_continuous(target_name):\n",
        "    predictors = binary_and_continuous_columns.drop(target_name)\n",
        "    X = df[predictors]\n",
        "    y = df[target_name]\n",
        "\n",
        "    # Set up RandomForest and cross-validation\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # RandomizedSearchCV for hyperparameter tuning\n",
        "    random_search = RandomizedSearchCV(rf, param_grid, n_iter=30, cv=cv, scoring='balanced_accuracy', n_jobs=-1, random_state=42)\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    # Best tuned model\n",
        "    best_rf = random_search.best_estimator_\n",
        "\n",
        "    # Cross-validated predictions using the tuned model\n",
        "    y_pred_cv = cross_val_predict(best_rf, X, y, cv=cv)\n",
        "\n",
        "    # Compute balanced accuracy\n",
        "    balanced_acc = balanced_accuracy_score(y, y_pred_cv)\n",
        "\n",
        "    return dict(zip(predictors, best_rf.feature_importances_)), balanced_acc\n",
        "\n",
        "# Compute tuned balanced accuracy with continuous variables\n",
        "tuned_results_with_continuous = {target: compute_tuned_accuracy_with_continuous(target) for target in binary_and_continuous_columns if target in binary_columns}\n",
        "\n",
        "# Separate the feature importances and balanced accuracies into two DataFrames\n",
        "tuned_importance_df_with_continuous = pd.DataFrame({target: tuned_results_with_continuous[target][0] for target in binary_columns}).T\n",
        "tuned_balanced_accuracies_with_continuous = {target: tuned_results_with_continuous[target][1] for target in binary_columns}\n",
        "tuned_balanced_accuracy_df_with_continuous = pd.DataFrame.from_dict(tuned_balanced_accuracies_with_continuous, orient='index', columns=['Adjusted Balanced Accuracy'])\n",
        "\n",
        "tuned_importance_df_sorted_with_continuous = tuned_importance_df_with_continuous.apply(lambda row: row.sort_values(ascending=False), axis=1)\n",
        "\n",
        "# Output the final DataFrames\n",
        "print(\"Tuned Feature Importances:\\n\", tuned_importance_df_sorted_with_continuous)\n",
        "print(\"\\nTuned Balanced Accuracy:\\n\", tuned_balanced_accuracy_df_with_continuous)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LwLIuzyib08",
        "outputId": "a8ec9d98-362d-4457-8f12-f1e930f74e43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Feature Importances:\n",
            "                    caramel  chocolate  crispedricewafer    fruity      hard  \\\n",
            "chocolate         0.010642        NaN          0.011811  0.366652  0.019740   \n",
            "fruity            0.046265   0.547767          0.000000       NaN  0.051303   \n",
            "caramel                NaN   0.010334          0.039703  0.033515  0.024455   \n",
            "peanutyalmondy    0.010226   0.006266          0.020801  0.008178  0.000000   \n",
            "nougat            0.204385   0.026460          0.008357  0.015392  0.001401   \n",
            "crispedricewafer  0.036639   0.003008               NaN  0.000000  0.000000   \n",
            "hard              0.010838   0.075100          0.000951  0.168695       NaN   \n",
            "pluribus          0.081530   0.091013          0.024926  0.059408  0.026977   \n",
            "\n",
            "                    nougat  peanutyalmondy  pluribus  pricepercent  \\\n",
            "chocolate         0.011966        0.026648  0.033424      0.118534   \n",
            "fruity            0.011144        0.039166  0.005296      0.047640   \n",
            "caramel           0.105702        0.021846  0.038481      0.172555   \n",
            "peanutyalmondy    0.063207             NaN  0.015305      0.264965   \n",
            "nougat                 NaN        0.114089  0.066942      0.080132   \n",
            "crispedricewafer  0.000000        0.020154  0.007089      0.338159   \n",
            "hard              0.001343        0.008290  0.033847      0.211580   \n",
            "pluribus          0.065784        0.024784       NaN      0.188768   \n",
            "\n",
            "                  sugarpercent  winpercent  \n",
            "chocolate             0.063724    0.336858  \n",
            "fruity                0.102630    0.148789  \n",
            "caramel               0.224234    0.329175  \n",
            "peanutyalmondy        0.027645    0.583406  \n",
            "nougat                0.173510    0.309333  \n",
            "crispedricewafer      0.060174    0.534777  \n",
            "hard                  0.193944    0.295412  \n",
            "pluribus              0.214144    0.222667  \n",
            "\n",
            "Tuned Balanced Accuracy:\n",
            "                   Adjusted Balanced Accuracy\n",
            "chocolate                           0.901182\n",
            "fruity                              0.864782\n",
            "caramel                             0.550805\n",
            "peanutyalmondy                      0.764588\n",
            "nougat                              0.500000\n",
            "crispedricewafer                    0.571429\n",
            "hard                                0.592857\n",
            "pluribus                            0.667683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IacPYh08lr3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict, StratifiedKFold , LeaveOneOut\n",
        "from sklearn.metrics import balanced_accuracy_score , make_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "def get_features(df, exclude_feature='bar', continuous_features=None):\n",
        "    \"\"\"Prepare feature columns, excluding the specified feature.\"\"\"\n",
        "    binary_columns = df.select_dtypes(include='int64').columns.drop(exclude_feature)\n",
        "    return binary_columns.append(pd.Index(continuous_features))\n",
        "\n",
        "\n",
        "def get_hyperparameter_grid():\n",
        "    \"\"\"Define the hyperparameter grid for RandomForestClassifier.\"\"\"\n",
        "\n",
        "    params = {\n",
        "        'n_estimators': [50, 100, 200, 500],\n",
        "        'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2', None]\n",
        "    }\n",
        "    params_overfitting = {\n",
        "        'n_estimators': [50, 100, 200, 500],  # Lower upper limit of estimators\n",
        "        'max_depth': [10, 15, 20],  # Limit maximum depth of trees\n",
        "        'min_samples_split': [5, 10, 15],  # Require more samples to split a node\n",
        "        'min_samples_leaf': [4, 6, 8],  # Require more samples per leaf\n",
        "        'max_features': ['sqrt', 'log2']  # Limit number of features considered at each split\n",
        "    }\n",
        "\n",
        "    param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['liblinear', 'saga'],  # Only solvers that support l1 and elasticnet\n",
        "    'max_iter': [100, 200, 500]\n",
        "    }\n",
        "\n",
        "    return param_grid\n",
        "\n",
        "\n",
        "\n",
        "def compute_tuned_feature_importance_and_accuracy(df, predictors, target_name, param_grid):\n",
        "    \"\"\"Compute tuned feature importances and balanced accuracy for a specific target.\"\"\"\n",
        "    X = df[predictors]\n",
        "    y = df[target_name]\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    log_reg = LogisticRegression(random_state=42)\n",
        "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "    # Custom scorer for balanced accuracy with adjustment\n",
        "    adjusted_balanced_accuracy_scorer = make_scorer(balanced_accuracy_score, adjusted=True)\n",
        "    random_search = RandomizedSearchCV(log_reg, param_grid, n_iter=100, cv=cv,\n",
        "                                       scoring= adjusted_balanced_accuracy_scorer, n_jobs=-1,\n",
        "                                       random_state=42)\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    best_rf = random_search.best_estimator_\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    #cv = LeaveOneOut ()\n",
        "    y_pred_cv = cross_val_predict(best_rf, X, y, cv=cv)\n",
        "\n",
        "    return dict(zip(predictors, best_rf.feature_importances_)), \\\n",
        "           balanced_accuracy_score(y, y_pred_cv,adjusted=True)\n",
        "\n",
        "\n",
        "def run_analysis(file_path, exclude_feature='bar', continuous_features=None):\n",
        "    \"\"\"Run analysis to compute tuned feature importances and balanced accuracy.\"\"\"\n",
        "    df = load_dataset(file_path)\n",
        "    all_features = get_features(df, exclude_feature=exclude_feature,\n",
        "                                continuous_features=continuous_features)\n",
        "    param_grid = get_hyperparameter_grid()\n",
        "\n",
        "    results = {target: compute_tuned_feature_importance_and_accuracy(\n",
        "        df, all_features.drop(target), target, param_grid)\n",
        "        for target in all_features if target in df.select_dtypes(include='int64').columns}\n",
        "\n",
        "    tuned_importance_df = pd.DataFrame({target: results[target][0] for target in results}).T\n",
        "    tuned_balanced_accuracy_df = pd.DataFrame.from_dict(\n",
        "        {target: results[target][1] for target in results}, orient='index',\n",
        "        columns=['Adjusted Balanced Accuracy'])\n",
        "\n",
        "    tuned_importance_df_sorted = tuned_importance_df.apply(lambda row: row.sort_values(ascending=False), axis=1)\n",
        "\n",
        "    return tuned_importance_df_sorted, tuned_balanced_accuracy_df\n",
        "\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"RandomForest Hyperparameter Tuning\")\n",
        "    parser.add_argument('file_path', type=str, help='Path to the dataset CSV file')\n",
        "    parser.add_argument('--exclude_feature', type=str, default='bar', help='Feature to exclude')\n",
        "    parser.add_argument('--continuous_features', nargs='+', default=['sugarpercent', 'pricepercent', 'winpercent'], help='List of continuous features')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    importance_df, accuracy_df = run_analysis(args.file_path, args.exclude_feature, args.continuous_features)\n",
        "\n",
        "    print(\"Tuned Feature Importances:\\n\", importance_df)\n",
        "    print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "TVmrnPo6lsG4",
        "outputId": "1a1bb388-9796-42e3-8d69-d3d79b21840d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    import argparse\\n\\n    parser = argparse.ArgumentParser(description=\"RandomForest Hyperparameter Tuning\")\\n    parser.add_argument(\\'file_path\\', type=str, help=\\'Path to the dataset CSV file\\')\\n    parser.add_argument(\\'--exclude_feature\\', type=str, default=\\'bar\\', help=\\'Feature to exclude\\')\\n    parser.add_argument(\\'--continuous_features\\', nargs=\\'+\\', default=[\\'sugarpercent\\', \\'pricepercent\\', \\'winpercent\\'], help=\\'List of continuous features\\')\\n\\n    args = parser.parse_args()\\n    importance_df, accuracy_df = run_analysis(args.file_path, args.exclude_feature, args.continuous_features)\\n\\n    print(\"Tuned Feature Importances:\\n\", importance_df)\\n    print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict, StratifiedKFold\n",
        "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "def get_features(df, exclude_feature='bar', continuous_features=None):\n",
        "    \"\"\"Prepare feature columns, excluding the specified feature.\"\"\"\n",
        "    binary_columns = df.select_dtypes(include='int64').columns.drop(exclude_feature)\n",
        "    return binary_columns.append(pd.Index(continuous_features))\n",
        "\n",
        "\n",
        "def get_hyperparameter_grid():\n",
        "    \"\"\"Define the hyperparameter grid for LogisticRegression.\"\"\"\n",
        "    return {\n",
        "        'penalty': ['l2', 'none','l1'],\n",
        "        'C': [0.01, 0.1, 1, 10, 100,1000],\n",
        "        'solver': ['lbfgs', 'liblinear'],\n",
        "        'max_iter': [100, 200, 500,1000]\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_tuned_feature_importance_and_accuracy(df, predictors, target_name, param_grid):\n",
        "    \"\"\"Compute tuned feature importances and balanced accuracy for a specific target.\"\"\"\n",
        "    X = df[predictors]\n",
        "    y = df[target_name]\n",
        "\n",
        "    log_reg = LogisticRegression(random_state=42)\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    adjusted_balanced_accuracy_scorer = make_scorer(balanced_accuracy_score, adjusted=True)\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        log_reg, param_grid, n_iter=100, cv=cv,\n",
        "        scoring=adjusted_balanced_accuracy_scorer, n_jobs=-1, random_state=42)\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    best_log_reg = random_search.best_estimator_\n",
        "    y_pred_cv = cross_val_predict(best_log_reg, X, y, cv=cv)\n",
        "\n",
        "    feature_importance = np.abs(best_log_reg.coef_).flatten()\n",
        "    feature_importance_dict = dict(zip(predictors, feature_importance))\n",
        "    balanced_accuracy = balanced_accuracy_score(y, y_pred_cv , adjusted=True)\n",
        "\n",
        "    print (\"best_log_reg\" , best_log_reg)\n",
        "    return feature_importance_dict, balanced_accuracy\n",
        "\n",
        "\n",
        "def run_analysis(file_path, exclude_feature='bar', continuous_features=None):\n",
        "    \"\"\"Run analysis to compute tuned feature importances and balanced accuracy.\"\"\"\n",
        "    df = load_dataset(file_path)\n",
        "    df [\"winpercent\"] = df [\"winpercent\"] / 100\n",
        "    c1  = df [\"chocolate\"] == 0\n",
        "    c2  = df [\"fruity\"]   == 0\n",
        "    c   = c1 & c2\n",
        "    df [\"other\"] = np.where (c , 1, 0)\n",
        "    all_features = get_features(df, exclude_feature=exclude_feature,\n",
        "                                continuous_features=continuous_features)\n",
        "    param_grid = get_hyperparameter_grid()\n",
        "\n",
        "    results = {target: compute_tuned_feature_importance_and_accuracy(\n",
        "        df, all_features.drop(target), target, param_grid)\n",
        "        for target in all_features if target in df.select_dtypes(include='int64').columns}\n",
        "\n",
        "    tuned_importance_df = pd.DataFrame({target: results[target][0] for target in results}).T\n",
        "    tuned_balanced_accuracy_df = pd.DataFrame.from_dict(\n",
        "        {target: results[target][1] for target in results}, orient='index',\n",
        "        columns=['Adjusted Balanced Accuracy'])\n",
        "\n",
        "    tuned_importance_df_sorted = tuned_importance_df.apply(lambda row: row.sort_values(ascending=False), axis=1)\n",
        "\n",
        "    return tuned_importance_df_sorted, tuned_balanced_accuracy_df\n",
        "\n",
        "\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Logistic Regression Hyperparameter Tuning\")\n",
        "    parser.add_argument('file_path', type=str, help='Path to the dataset CSV file')\n",
        "    parser.add_argument('--exclude_feature', type=str, default='bar', help='Feature to exclude')\n",
        "    parser.add_argument('--continuous_features', nargs='+', default=['sugarpercent', 'pricepercent', 'winpercent'], help='List of continuous features')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    importance_df, accuracy_df = run_analysis(args.file_path, args.exclude_feature, args.continuous_features)\n",
        "\n",
        "    print(\"Tuned Feature Importances:\\n\", importance_df)\n",
        "    print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "5PgkCvg55WmA",
        "outputId": "b2a314e8-bf68-4805-f9cb-276ec3088688"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    import argparse\\n\\n    parser = argparse.ArgumentParser(description=\"Logistic Regression Hyperparameter Tuning\")\\n    parser.add_argument(\\'file_path\\', type=str, help=\\'Path to the dataset CSV file\\')\\n    parser.add_argument(\\'--exclude_feature\\', type=str, default=\\'bar\\', help=\\'Feature to exclude\\')\\n    parser.add_argument(\\'--continuous_features\\', nargs=\\'+\\', default=[\\'sugarpercent\\', \\'pricepercent\\', \\'winpercent\\'], help=\\'List of continuous features\\')\\n\\n    args = parser.parse_args()\\n    importance_df, accuracy_df = run_analysis(args.file_path, args.exclude_feature, args.continuous_features)\\n\\n    print(\"Tuned Feature Importances:\\n\", importance_df)\\n    print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict, StratifiedKFold\n",
        "from sklearn.metrics import balanced_accuracy_score, make_scorer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "def get_features(df, exclude_feature='bar', continuous_features=None):\n",
        "    \"\"\"Prepare feature columns, excluding the specified feature.\"\"\"\n",
        "    binary_columns = df.select_dtypes(include='int64').columns.drop(exclude_feature)\n",
        "    return binary_columns.append(pd.Index(continuous_features))\n",
        "\n",
        "\n",
        "def get_hyperparameter_grid():\n",
        "    \"\"\"Define the hyperparameter grid for LogisticRegression.\"\"\"\n",
        "    return {\n",
        "        'poly__degree': [1, 2, 3],  # Polynomial degree\n",
        "        'log_reg__penalty': ['l2', 'none','l1'],\n",
        "        'log_reg__C': [0.01, 0.1, 1, 10, 100,1000],\n",
        "        'log_reg__solver': ['lbfgs', 'liblinear'],\n",
        "        'log_reg__max_iter': [100, 200, 500,1000]\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_tuned_feature_importance_and_accuracy(df, predictors, target_name, param_grid):\n",
        "    \"\"\"Compute tuned feature importances and balanced accuracy for a specific target.\"\"\"\n",
        "    X = df[predictors]\n",
        "    y = df[target_name]\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('poly', PolynomialFeatures()),  # Polynomial features transformation\n",
        "        ('log_reg', LogisticRegression(random_state=42))\n",
        "    ])\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    adjusted_balanced_accuracy_scorer = make_scorer(balanced_accuracy_score, adjusted=True)\n",
        "\n",
        "    random_search = RandomizedSearchCV(\n",
        "        pipeline, param_grid, n_iter=20, cv=cv,\n",
        "        scoring=adjusted_balanced_accuracy_scorer, n_jobs=-1, random_state=42)\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    best_pipeline = random_search.best_estimator_\n",
        "    y_pred_cv = cross_val_predict(best_pipeline, X, y, cv=cv)\n",
        "\n",
        "    log_reg = best_pipeline.named_steps['log_reg']\n",
        "    poly = best_pipeline.named_steps['poly']\n",
        "    feature_importance = np.abs(log_reg.coef_).flatten()\n",
        "    feature_names = poly.get_feature_names_out(predictors)\n",
        "    feature_importance_dict = dict(zip(feature_names, feature_importance))\n",
        "    balanced_accuracy = balanced_accuracy_score(y, y_pred_cv,adjusted=True)\n",
        "\n",
        "    return feature_importance_dict, balanced_accuracy\n",
        "\n",
        "\n",
        "def run_analysis(file_path, exclude_feature='bar', continuous_features=None):\n",
        "    \"\"\"Run analysis to compute tuned feature importances and balanced accuracy.\"\"\"\n",
        "    df = load_dataset(file_path)\n",
        "    df [\"winpercent\"] = df [\"winpercent\"] / 100\n",
        "    c1  = df [\"chocolate\"] == 0\n",
        "    c2  = df [\"fruity\"]   == 0\n",
        "    c   = c1 & c2\n",
        "    df [\"other\"] = np.where (c , 1, 0)\n",
        "    all_features = get_features(df, exclude_feature=exclude_feature,\n",
        "                                continuous_features=continuous_features)\n",
        "    param_grid = get_hyperparameter_grid()\n",
        "\n",
        "    results = {target: compute_tuned_feature_importance_and_accuracy(\n",
        "        df, all_features.drop(target), target, param_grid)\n",
        "        for target in all_features if target in df.select_dtypes(include='int64').columns}\n",
        "\n",
        "    tuned_importance_df = pd.DataFrame({target: results[target][0] for target in results}).T\n",
        "    tuned_balanced_accuracy_df = pd.DataFrame.from_dict(\n",
        "        {target: results[target][1] for target in results}, orient='index',\n",
        "        columns=['Adjusted Balanced Accuracy'])\n",
        "\n",
        "    tuned_importance_df_sorted = tuned_importance_df.apply(lambda row: row.sort_values(ascending=False), axis=1)\n",
        "\n",
        "    return tuned_importance_df_sorted, tuned_balanced_accuracy_df\n",
        "\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Logistic Regression Hyperparameter Tuning with Polynomial Features\")\n",
        "    parser.add_argument('file_path', type=str, help='Path to the dataset CSV file')\n",
        "    parser.add_argument('--exclude_feature', type=str, default='bar', help='Feature to exclude')\n",
        "    parser.add_argument('--continuous_features', nargs='+', default=['sugarpercent', 'pricepercent', 'winpercent'], help='List of continuous features')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    importance_df, accuracy_df = run_analysis(args.file_path, args.exclude_feature, args.continuous_features)\n",
        "\n",
        "    print(\"Tuned Feature Importances:\\n\", importance_df)\n",
        "    print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "e6T7sQFnQx01",
        "outputId": "81a10859-1811-499c-eaa2-91584aa3d5a6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n    import argparse\\n\\n    parser = argparse.ArgumentParser(description=\"Logistic Regression Hyperparameter Tuning with Polynomial Features\")\\n    parser.add_argument(\\'file_path\\', type=str, help=\\'Path to the dataset CSV file\\')\\n    parser.add_argument(\\'--exclude_feature\\', type=str, default=\\'bar\\', help=\\'Feature to exclude\\')\\n    parser.add_argument(\\'--continuous_features\\', nargs=\\'+\\', default=[\\'sugarpercent\\', \\'pricepercent\\', \\'winpercent\\'], help=\\'List of continuous features\\')\\n\\n    args = parser.parse_args()\\n    importance_df, accuracy_df = run_analysis(args.file_path, args.exclude_feature, args.continuous_features)\\n\\n    print(\"Tuned Feature Importances:\\n\", importance_df)\\n    print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cat_interdependent import run_analysis\n",
        "\n",
        "\n",
        "dataset_path = 'candy-data.csv'  # Adjust the path to your dataset\n",
        "\n",
        "continuous_features = ['sugarpercent', 'pricepercent', 'winpercent']\n",
        "continuous_features = []\n",
        "\n",
        "importance_df, accuracy_df = run_analysis(\n",
        "    dataset_path, exclude_feature='bar', continuous_features=continuous_features)\n",
        "\n",
        "print(\"Tuned Feature Importances:\\n\", importance_df)\n",
        "print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBzGwI9Rl8kX",
        "outputId": "ebe55910-29a1-48d2-a960-b0cb76fd9096"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Feature Importances:\n",
            "                    caramel  chocolate  crispedricewafer    fruity      hard  \\\n",
            "chocolate         0.035575        NaN          0.072477  0.611252  0.081340   \n",
            "fruity            0.087206   0.697198          0.000000       NaN  0.105499   \n",
            "caramel                NaN   0.130167          0.126342  0.172397  0.072941   \n",
            "peanutyalmondy    0.146773   0.247790          0.107190  0.181951  0.038995   \n",
            "nougat            0.220773   0.222022          0.166490  0.037028  0.018673   \n",
            "crispedricewafer  0.193589   0.230000               NaN  0.026449  0.016760   \n",
            "hard              0.111925   0.273881          0.000000  0.435349       NaN   \n",
            "pluribus          0.145502   0.295507          0.077730  0.146063  0.066086   \n",
            "\n",
            "                    nougat  peanutyalmondy  pluribus  \n",
            "chocolate         0.040939        0.091770  0.066647  \n",
            "fruity            0.012491        0.073652  0.023953  \n",
            "caramel           0.213021        0.129181  0.155951  \n",
            "peanutyalmondy    0.178522             NaN  0.098779  \n",
            "nougat                 NaN        0.197115  0.137898  \n",
            "crispedricewafer  0.202030        0.166342  0.164830  \n",
            "hard              0.001209        0.024044  0.153593  \n",
            "pluribus          0.162033        0.107079       NaN  \n",
            "\n",
            "Tuned Balanced Accuracy:\n",
            "                   Adjusted Balanced Accuracy\n",
            "chocolate                           0.892736\n",
            "fruity                              0.888578\n",
            "caramel                             0.564889\n",
            "peanutyalmondy                      0.600604\n",
            "nougat                              0.772894\n",
            "crispedricewafer                    0.636447\n",
            "hard                                0.557143\n",
            "pluribus                            0.669346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_features(df, exclude_feature='bar', continuous_features=['sugarpercent', 'pricepercent', 'winpercent'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sku9j-U6om_9",
        "outputId": "9a5cff22-4c53-4734-ce7e-b60681f3297b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['chocolate', 'fruity', 'caramel', 'peanutyalmondy', 'nougat',\n",
              "       'crispedricewafer', 'hard', 'pluribus', 'sugarpercent', 'pricepercent',\n",
              "       'winpercent'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cat_interdependent import run_analysis\n",
        "\n",
        "\n",
        "dataset_path = 'candy-data.csv'  # Adjust the path to your dataset\n",
        "\n",
        "continuous_features = ['sugarpercent', 'pricepercent', 'winpercent']\n",
        "#continuous_features = []\n",
        "\n",
        "importance_df, accuracy_df = run_analysis(\n",
        "    dataset_path, exclude_feature='bar', continuous_features=continuous_features)\n",
        "\n",
        "print(\"Tuned Feature Importances:\\n\", importance_df)\n",
        "print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjYIbKLNoFyz",
        "outputId": "c5369726-ec5f-4b4a-f456-b36474f8e4b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Feature Importances:\n",
            "                    caramel  chocolate  crispedricewafer    fruity      hard  \\\n",
            "chocolate         0.010642        NaN          0.011811  0.366652  0.019740   \n",
            "fruity            0.046265   0.547767          0.000000       NaN  0.051303   \n",
            "caramel                NaN   0.010334          0.039703  0.033515  0.024455   \n",
            "peanutyalmondy    0.010226   0.006266          0.020801  0.008178  0.000000   \n",
            "nougat            0.204385   0.026460          0.008357  0.015392  0.001401   \n",
            "crispedricewafer  0.036639   0.003008               NaN  0.000000  0.000000   \n",
            "hard              0.010838   0.075100          0.000951  0.168695       NaN   \n",
            "pluribus          0.081530   0.091013          0.024926  0.059408  0.026977   \n",
            "\n",
            "                    nougat  peanutyalmondy  pluribus  pricepercent  \\\n",
            "chocolate         0.011966        0.026648  0.033424      0.118534   \n",
            "fruity            0.011144        0.039166  0.005296      0.047640   \n",
            "caramel           0.105702        0.021846  0.038481      0.172555   \n",
            "peanutyalmondy    0.063207             NaN  0.015305      0.264965   \n",
            "nougat                 NaN        0.114089  0.066942      0.080132   \n",
            "crispedricewafer  0.000000        0.020154  0.007089      0.338159   \n",
            "hard              0.001343        0.008290  0.033847      0.211580   \n",
            "pluribus          0.065784        0.024784       NaN      0.188768   \n",
            "\n",
            "                  sugarpercent  winpercent  \n",
            "chocolate             0.063724    0.336858  \n",
            "fruity                0.102630    0.148789  \n",
            "caramel               0.224234    0.329175  \n",
            "peanutyalmondy        0.027645    0.583406  \n",
            "nougat                0.173510    0.309333  \n",
            "crispedricewafer      0.060174    0.534777  \n",
            "hard                  0.193944    0.295412  \n",
            "pluribus              0.214144    0.222667  \n",
            "\n",
            "Tuned Balanced Accuracy:\n",
            "                   Adjusted Balanced Accuracy\n",
            "chocolate                           0.901182\n",
            "fruity                              0.864782\n",
            "caramel                             0.550805\n",
            "peanutyalmondy                      0.764588\n",
            "nougat                              0.500000\n",
            "crispedricewafer                    0.571429\n",
            "hard                                0.592857\n",
            "pluribus                            0.667683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/candy-data.csv'  # Adjust the path to your dataset\n",
        "\n",
        "continuous_features = ['sugarpercent', 'pricepercent', 'winpercent']\n",
        "continuous_features = []\n",
        "\n",
        "importance_df, accuracy_df  = run_analysis(\n",
        "    dataset_path, exclude_feature=[], continuous_features=continuous_features)\n",
        "\n",
        "print(\"Tuned Feature Importances:\\n\", importance_df)\n",
        "print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eKQ7Juc1XDJ",
        "outputId": "bed65fa2-4f2a-4d2c-a515-8a6e80f6a415"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.69920635 0.94920635        nan        nan 0.94920635\n",
            " 0.97142857        nan 0.97142857 0.94920635        nan 0.\n",
            " 0.94920635        nan 0.50634921        nan 0.97142857 0.69920635\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.88420635 0.94920635        nan        nan 0.97142857\n",
            " 0.97142857        nan 0.97142857 0.97142857        nan 0.\n",
            " 0.97142857        nan 0.51428571        nan 0.97142857 0.84198413\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan  0.         -0.04285714         nan         nan  0.11904762\n",
            "  0.1                nan -0.02857143  0.11428571         nan  0.\n",
            "  0.04761905         nan  0.                 nan  0.11904762  0.\n",
            "         nan         nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan  0.         -0.01428571         nan         nan  0.12952381\n",
            " -0.05619048         nan  0.         -0.02857143         nan  0.\n",
            " -0.02857143         nan  0.                 nan  0.12952381  0.\n",
            "         nan         nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.         0.                nan        nan 0.58666667\n",
            " 0.58666667        nan 0.         0.38666667        nan 0.\n",
            " 0.38666667        nan 0.                nan 0.58666667 0.\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.         0.                nan        nan 0.18666667\n",
            " 0.18666667        nan 0.         0.08666667        nan 0.\n",
            " 0.08666667        nan 0.                nan 0.18666667 0.\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.         0.                nan        nan 0.03333333\n",
            " 0.00952381        nan 0.         0.07619048        nan 0.\n",
            " 0.                nan 0.                nan 0.03333333 0.\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.38       0.75846154        nan        nan 0.79846154\n",
            " 0.79846154        nan 0.88846154 0.84846154        nan 0.\n",
            " 0.79846154        nan 0.                nan 0.79846154 0.1\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.33611111 0.46944444        nan        nan 0.42222222\n",
            " 0.46944444        nan 0.51388889 0.42222222        nan 0.\n",
            " 0.44722222        nan 0.29166667        nan 0.4        0.45277778\n",
            "        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Feature Importances:\n",
            "                          1       bar  bar other  bar other^2  bar pluribus  \\\n",
            "chocolate         0.005188  0.003054        NaN          NaN           NaN   \n",
            "fruity            1.519387  0.456415   0.013749     0.013749           0.0   \n",
            "caramel           0.496446  0.055549   0.352581     0.352581           0.0   \n",
            "peanutyalmondy    0.661497  0.115362   0.653396     0.653396           0.0   \n",
            "nougat            1.360241  0.789387   0.596136     0.596136           0.0   \n",
            "crispedricewafer  1.338748  0.390238   0.002751     0.002751           0.0   \n",
            "hard              0.542782  0.434670   0.035955          NaN           0.0   \n",
            "bar               0.000000       NaN        NaN          NaN           NaN   \n",
            "pluribus          0.339768  3.625760        NaN          NaN           NaN   \n",
            "other             1.058268  0.214208        NaN          NaN           0.0   \n",
            "\n",
            "                  bar pluribus other  bar pluribus^2     bar^2  bar^2 other  \\\n",
            "chocolate                        NaN             NaN       NaN          NaN   \n",
            "fruity                           0.0             0.0  0.456415     0.013749   \n",
            "caramel                          0.0             0.0  0.055549     0.352581   \n",
            "peanutyalmondy                   0.0             0.0  0.115362     0.653396   \n",
            "nougat                           0.0             0.0  0.789387     0.596136   \n",
            "crispedricewafer                 0.0             0.0  0.390238     0.002751   \n",
            "hard                             NaN             NaN  0.434670          NaN   \n",
            "bar                              NaN             NaN       NaN          NaN   \n",
            "pluribus                         NaN             NaN       NaN          NaN   \n",
            "other                            NaN             0.0  0.214208          NaN   \n",
            "\n",
            "                  bar^2 pluribus  ...  peanutyalmondy^2 nougat  \\\n",
            "chocolate                    NaN  ...                      NaN   \n",
            "fruity                       0.0  ...                 0.015408   \n",
            "caramel                      0.0  ...                 0.764846   \n",
            "peanutyalmondy               0.0  ...                      NaN   \n",
            "nougat                       0.0  ...                      NaN   \n",
            "crispedricewafer             0.0  ...                 0.171070   \n",
            "hard                         NaN  ...                      NaN   \n",
            "bar                          NaN  ...                      NaN   \n",
            "pluribus                     NaN  ...                      NaN   \n",
            "other                        0.0  ...                 0.225552   \n",
            "\n",
            "                  peanutyalmondy^2 other  peanutyalmondy^2 pluribus  \\\n",
            "chocolate                            NaN                        NaN   \n",
            "fruity                          0.048031                   0.071404   \n",
            "caramel                         0.406803                   0.332791   \n",
            "peanutyalmondy                       NaN                        NaN   \n",
            "nougat                          0.519832                   0.083448   \n",
            "crispedricewafer                0.010658                   0.175159   \n",
            "hard                                 NaN                        NaN   \n",
            "bar                                  NaN                        NaN   \n",
            "pluribus                             NaN                        NaN   \n",
            "other                                NaN                   0.146989   \n",
            "\n",
            "                  peanutyalmondy^3  pluribus  pluribus other  \\\n",
            "chocolate                      NaN  3.760599             NaN   \n",
            "fruity                    0.396153  0.465868        0.805906   \n",
            "caramel                   0.570912  0.264796        0.032657   \n",
            "peanutyalmondy                 NaN  0.161738        0.589457   \n",
            "nougat                    0.078412  0.563894        0.184776   \n",
            "crispedricewafer          0.435953  0.126992        0.189399   \n",
            "hard                           NaN  0.366718        0.130717   \n",
            "bar                            NaN  3.324393             NaN   \n",
            "pluribus                       NaN       NaN             NaN   \n",
            "other                     0.042100  0.332148             NaN   \n",
            "\n",
            "                  pluribus other^2  pluribus^2  pluribus^2 other  pluribus^3  \n",
            "chocolate                      NaN         NaN               NaN         NaN  \n",
            "fruity                    0.805906    0.465868          0.805906    0.465868  \n",
            "caramel                   0.032657    0.264796          0.032657    0.264796  \n",
            "peanutyalmondy            0.589457    0.161738          0.589457    0.161738  \n",
            "nougat                    0.184776    0.563894          0.184776    0.563894  \n",
            "crispedricewafer          0.189399    0.126992          0.189399    0.126992  \n",
            "hard                           NaN    0.366718               NaN         NaN  \n",
            "bar                            NaN         NaN               NaN         NaN  \n",
            "pluribus                       NaN         NaN               NaN         NaN  \n",
            "other                          NaN    0.332148               NaN    0.332148  \n",
            "\n",
            "[10 rows x 286 columns]\n",
            "\n",
            "Tuned Balanced Accuracy:\n",
            "                   Adjusted Balanced Accuracy\n",
            "chocolate                           0.972973\n",
            "fruity                              0.973684\n",
            "caramel                             0.101610\n",
            "peanutyalmondy                      0.143863\n",
            "nougat                              0.558608\n",
            "crispedricewafer                    0.272894\n",
            "hard                                0.076190\n",
            "bar                                 0.889881\n",
            "pluribus                            0.512195\n",
            "other                               1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan 0.  0.1 nan nan 1.  1.  nan 1.  0.9 nan 0.  0.9 nan 0.  nan 1.  0.\n",
            " nan nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/candy-data.csv'  # Adjust the path to your dataset\n",
        "\n",
        "continuous_features = ['sugarpercent', 'pricepercent', 'winpercent']\n",
        "#continuous_features = []\n",
        "\n",
        "importance_df, accuracy_df = run_analysis(\n",
        "    dataset_path, exclude_feature=[], continuous_features=continuous_features)\n",
        "\n",
        "print(\"Tuned Feature Importances:\\n\", importance_df)\n",
        "print(\"\\nTuned Balanced Accuracy:\\n\", accuracy_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J215k09QuLNe",
        "outputId": "51c47fe4-de59-4818-eb9e-be9f58fc53ee"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.75277778 0.94920635        nan        nan 0.92920635\n",
            " 0.8847619         nan 0.97142857 0.94920635        nan 0.\n",
            " 0.94920635        nan 0.58849206        nan 0.92920635 0.72420635\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.88420635 0.94920635        nan        nan 0.97142857\n",
            " 0.94920635        nan 0.97142857 0.94920635        nan 0.\n",
            " 0.97142857        nan 0.21428571        nan 0.97142857 0.86198413\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan  0.          0.03809524         nan         nan  0.12095238\n",
            "  0.08761905         nan -0.02857143  0.1952381          nan  0.\n",
            "  0.1952381          nan  0.                 nan  0.19809524  0.\n",
            "         nan         nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan  0.          0.                 nan         nan  0.18666667\n",
            "  0.2152381          nan  0.         -0.01428571         nan  0.\n",
            " -0.01428571         nan  0.                 nan  0.34571429  0.\n",
            "         nan         nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.         0.                nan        nan 0.57416667\n",
            " 0.5475            nan 0.         0.38666667        nan 0.\n",
            " 0.38666667        nan 0.                nan 0.5475     0.\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.         0.                nan        nan 0.2475\n",
            " 0.17333333        nan 0.         0.08666667        nan 0.\n",
            " 0.18666667        nan 0.                nan 0.3225     0.\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan  0.          0.                 nan         nan -0.03809524\n",
            "  0.17619048         nan  0.         -0.02857143         nan  0.\n",
            " -0.01428571         nan  0.                 nan -0.02857143  0.\n",
            "         nan         nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.48       0.85384615        nan        nan 0.88461538\n",
            " 0.79717949        nan 0.88846154 0.85384615        nan 0.\n",
            " 0.85384615        nan 0.                nan 0.82179487 0.19\n",
            "        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.33611111 0.49166667        nan        nan 0.40555556\n",
            " 0.43055556        nan 0.49166667 0.425             nan 0.\n",
            " 0.425             nan 0.29166667        nan 0.36666667 0.43333333\n",
            "        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Feature Importances:\n",
            "                          1       bar  bar other  bar other pricepercent  \\\n",
            "chocolate         0.511609  0.661454        NaN                     NaN   \n",
            "fruity            1.298833  0.272864   0.005178                0.003972   \n",
            "caramel           2.223207  8.407119   5.008732                     NaN   \n",
            "peanutyalmondy    4.727834  4.803084   4.884373                     NaN   \n",
            "nougat            1.223848  0.581072   0.476208                0.365252   \n",
            "crispedricewafer  5.744561  0.324956   0.000387                     NaN   \n",
            "hard              0.029046  2.111739        NaN                     NaN   \n",
            "bar               0.000000       NaN        NaN                     NaN   \n",
            "pluribus          0.000017  2.283980        NaN                     NaN   \n",
            "other             1.067434  0.113134        NaN                     NaN   \n",
            "\n",
            "                  bar other sugarpercent  bar other winpercent  bar other^2  \\\n",
            "chocolate                            NaN                   NaN          NaN   \n",
            "fruity                          0.002408              0.002397     0.005178   \n",
            "caramel                              NaN                   NaN          NaN   \n",
            "peanutyalmondy                       NaN                   NaN          NaN   \n",
            "nougat                          0.221437              0.220468     0.476208   \n",
            "crispedricewafer                     NaN                   NaN          NaN   \n",
            "hard                                 NaN                   NaN          NaN   \n",
            "bar                                  NaN                   NaN          NaN   \n",
            "pluribus                             NaN                   NaN          NaN   \n",
            "other                                NaN                   NaN          NaN   \n",
            "\n",
            "                  bar pluribus  bar pluribus other  bar pluribus pricepercent  \\\n",
            "chocolate                  NaN                 NaN                        NaN   \n",
            "fruity                     0.0                 0.0                        0.0   \n",
            "caramel                    0.0                 NaN                        NaN   \n",
            "peanutyalmondy             0.0                 NaN                        NaN   \n",
            "nougat                     0.0                 0.0                        0.0   \n",
            "crispedricewafer           0.0                 NaN                        NaN   \n",
            "hard                       NaN                 NaN                        NaN   \n",
            "bar                        NaN                 NaN                        NaN   \n",
            "pluribus                   NaN                 NaN                        NaN   \n",
            "other                      0.0                 NaN                        0.0   \n",
            "\n",
            "                  ...  sugarpercent pricepercent^2  sugarpercent winpercent  \\\n",
            "chocolate         ...                          NaN                      NaN   \n",
            "fruity            ...                     0.026998                 0.283803   \n",
            "caramel           ...                          NaN                10.423620   \n",
            "peanutyalmondy    ...                          NaN                 2.665187   \n",
            "nougat            ...                     0.623686                 0.172202   \n",
            "crispedricewafer  ...                          NaN                 5.922597   \n",
            "hard              ...                          NaN                      NaN   \n",
            "bar               ...                          NaN                      NaN   \n",
            "pluribus          ...                          NaN                      NaN   \n",
            "other             ...                     0.044936                 0.132635   \n",
            "\n",
            "                  sugarpercent winpercent^2  sugarpercent^2  \\\n",
            "chocolate                               NaN             NaN   \n",
            "fruity                             0.113007        0.314407   \n",
            "caramel                                 NaN       16.943905   \n",
            "peanutyalmondy                          NaN        5.221614   \n",
            "nougat                             0.166866        0.084944   \n",
            "crispedricewafer                        NaN        1.319485   \n",
            "hard                                    NaN             NaN   \n",
            "bar                                     NaN             NaN   \n",
            "pluribus                                NaN             NaN   \n",
            "other                              0.117839        0.069556   \n",
            "\n",
            "                  sugarpercent^2 pricepercent  sugarpercent^2 winpercent  \\\n",
            "chocolate                                 NaN                        NaN   \n",
            "fruity                               0.058402                   0.137965   \n",
            "caramel                                   NaN                        NaN   \n",
            "peanutyalmondy                            NaN                        NaN   \n",
            "nougat                               0.431661                   0.076707   \n",
            "crispedricewafer                          NaN                        NaN   \n",
            "hard                                      NaN                        NaN   \n",
            "bar                                       NaN                        NaN   \n",
            "pluribus                                  NaN                        NaN   \n",
            "other                                0.007977                   0.046552   \n",
            "\n",
            "                  sugarpercent^3  winpercent  winpercent^2  winpercent^3  \n",
            "chocolate                    NaN    0.932866           NaN           NaN  \n",
            "fruity                  0.169739    0.526169      0.206071      0.064837  \n",
            "caramel                      NaN    5.269776      6.484216           NaN  \n",
            "peanutyalmondy               NaN    1.044558     11.897109           NaN  \n",
            "nougat                  0.431853    0.379678      0.073603      0.267788  \n",
            "crispedricewafer             NaN    4.306827      6.972214           NaN  \n",
            "hard                         NaN    7.068554           NaN           NaN  \n",
            "bar                          NaN    0.000000           NaN           NaN  \n",
            "pluribus                     NaN    0.165910           NaN           NaN  \n",
            "other                   0.085649    0.123418      0.115909      0.147766  \n",
            "\n",
            "[10 rows x 559 columns]\n",
            "\n",
            "Tuned Balanced Accuracy:\n",
            "                   Adjusted Balanced Accuracy\n",
            "chocolate                           0.972973\n",
            "fruity                              0.973684\n",
            "caramel                             0.188129\n",
            "peanutyalmondy                      0.345070\n",
            "nougat                              0.545788\n",
            "crispedricewafer                    0.351648\n",
            "hard                                0.176190\n",
            "bar                                 0.889881\n",
            "pluribus                            0.491131\n",
            "other                               1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "45 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan 0.  0.2 nan nan 1.  1.  nan 1.  0.9 nan 0.  0.9 nan 0.  nan 1.  0.\n",
            " nan nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "class IterativeFeatureImportance(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, param_grid=None, n_iter=30):\n",
        "        self.param_grid = param_grid or {\n",
        "            'n_estimators': [50, 100, 200, 500],\n",
        "            'max_depth': [None, 10, 20, 30, 40, 50],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "        self.n_iter = n_iter\n",
        "        self.results_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for target in X.columns:\n",
        "            X_subset = X.drop(columns=[target])\n",
        "            y_subset = X[target]\n",
        "\n",
        "            rf = RandomForestClassifier(random_state=42)\n",
        "            random_search = RandomizedSearchCV(rf, self.param_grid, n_iter=self.n_iter, cv=5, scoring='balanced_accuracy', n_jobs=-1, random_state=42)\n",
        "            random_search.fit(X_subset, y_subset)\n",
        "            best_rf = random_search.best_estimator_\n",
        "\n",
        "            y_pred_cv = cross_val_predict(best_rf, X_subset, y_subset, cv=5)\n",
        "            balanced_acc = balanced_accuracy_score(y_subset, y_pred_cv)\n",
        "\n",
        "            self.results_[target] = {\n",
        "                'model': best_rf,\n",
        "                'feature_importances': best_rf.feature_importances_,\n",
        "                'balanced_accuracy': balanced_acc\n",
        "            }\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_results(self):\n",
        "        return self.results_\n"
      ],
      "metadata": {
        "id": "Mv5yO5OJqvcI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset and specify continuous features\n",
        "df = pd.read_csv('/content/candy-data.csv')  # Adjust path to dataset\n",
        "continuous_features = ['sugarpercent', 'pricepercent', 'winpercent']\n",
        "\n",
        "# Drop the feature that you don't want to include, e.g., 'bar'\n",
        "features_to_include = df.select_dtypes(include='int64').columns.drop('bar').tolist()\n",
        "all_features = features_to_include + continuous_features\n",
        "\n",
        "# Filter only the columns to include in the analysis\n",
        "X = df[all_features]\n",
        "\n",
        "# Initialize the iterative feature importance estimator\n",
        "iterative_importance = IterativeFeatureImportance()\n",
        "\n",
        "# Fit the estimator to the dataset\n",
        "iterative_importance.fit(X)\n",
        "\n",
        "# Get the results\n",
        "results = iterative_importance.get_results()\n",
        "\n",
        "# Display results\n",
        "for target, result in results.items():\n",
        "    print(f\"Target: {target}\")\n",
        "    print(f\"Balanced Accuracy: {result['balanced_accuracy']}\")\n",
        "    print(f\"Feature Importances: {result['feature_importances']}\\n\")\n"
      ],
      "metadata": {
        "id": "rXFZC3AYqxao"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}